---
layout: post
title:  "On generative LLMs"
categories: [] 
image: assets/images/chatgpt.png
---
I'm going to detox from generative LLMs for a while. To be clear, I think generative LLMs (erroneously referred to as "AI") are incredible tools, and I *do* believe there is merit in knowing how to prompt them to give you ever better responses. I'm not a luddite, and I'm not (too) scared by the rapid adoption of generative LLMs across nearly every field. It's kind of an "it's not you, it's me" thing. Generative LLMs are simply too good at generating responses that I can use to solve my problems without thinking deeply about the problem. And that's problematic for someone whose career is centered around clearly defining problems and then developing solutions to optimally solve them given the constraints. 

Problem-solving is a skill, and if you don't practice that skill, your ability to use it diminishes. When code I write fails an edge case, it is incredibly easy to ask ChatGPT to fix it for me. But every time I do this, I'm depriving myself of an opportunity to strengthen my problem-solving skills. If I let my problem-solving ability atrophy, how can I expect to be successful when encountering a problem that ChatGPT can't solve?

There is merit to generative LLMs. There is also merit to having strong problem-solving muscles. For now, I'm going to leave problem-solving to my own ability. Yes, this might slow me down at first, but I'm betting the long-term benefits will far outweigh the short-term frustration. So deep breath. 
Define the problem. 
Consider existing technologies/frameworks that could resolve it. 
Generate lists of constraints, test cases, and edge cases. 
Weigh multiple solutions and consider their tradeoffs. 
Implement. 
Iterate. 

And no, not a single word of this was generated using generative LLMs. 